# LLM Usage Statement

I have used perplexity.ai and the comet browser to 
- Used extensively to generate code for testing 
- Summarize concepts in the paper 
- Suggest alternate datasets for the paper 
- Make points more concise for the answers presented  

# Commercial LLM used 
Comet Version 140.0.7339.186 (Official Build) (arm64)
Perplexity build number: 20994
Student License 

# Tasks performed (not exhaustive)

- Section A
    - used llm extensively to generate code for testing 
    - summarise core idea of https://arxiv.org/pdf/2309.12307
    - compare againt using LoRA against RAG for very long content (context)
    - what is position interpolation when scaling context 
    - what are other efficient attention training mechanisms
    - what are Key Limitations of the approach ? 
    - softmax formula for attention 
    - Which techniques alter the architecture of attention layer during inference as a part of the adaption
- Section B
    - used llm extensively to generate code for testing 
    - Used llm using multiple prompts to generate a dataset for the multi language test 
    - tried to debug LongLoRA OOM issue when trying to increase context as mentioned in the paper 
    - 
