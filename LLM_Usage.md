# LLM Usage Statement

I have used perplexity.ai and the comet browser to 
- Summarize concepts in the paper 
- Suggest alternate datasets for the paper 
- Make points more concise for the answers presented  

# Model used 
Comet Version 140.0.7339.186 (Official Build) (arm64)
Perplexity build number: 20994


# Tasks performed (not exhaustive)

- summarise core idea of https://arxiv.org/pdf/2309.12307
- compare againt using LoRA against RAG for very long content (context)
- what is position interpolation when scaling context 
- what are other efficient attention training mechanisms
- what are Key Limitations of the approach ? 
- softmax formula for attention 
- Which techniques alter the architecture of attention layer during inference as a part of the adaption
- 
